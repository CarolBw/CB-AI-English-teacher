{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jvL8zyU-9E7i",
        "_lxEZbkA9Qmd",
        "cbCdAz7CoHnH",
        "PPk1EpnXFhs-",
        "N0DO-ZstF2MT",
        "Vr9vqvY_wFcD",
        "FXINEWY-CRxJ",
        "aGhZ8qBGAoV0",
        "n5nxVCmp-uQ2",
        "SvrhfIDFxQlb",
        "mwMdT5g9vZB9",
        "rtSKf8ATRUL-",
        "l8mzJRiovdA4"
      ],
      "toc_visible": true,
      "mount_file_id": "1c_CYGoummbwB9jAli4E_jEwPEN7D6g2X",
      "authorship_tag": "ABX9TyN796CEx7WrBaVc/x7MmP5f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CarolBw/CB-AI-English-teacher/blob/main/CB_AI_English_teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto MVP CB-AI English teacher.\n",
        "Trabalho de conclusão da Sprint Machine Learning & Analytics.\n",
        "Pós graduação em Ciencia de Dados, PUC-RJ.\n"
      ],
      "metadata": {
        "id": "i6kGnT7qughS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Visão Geral\n",
        "\n",
        "Este notebook contém o código para o desenvolvimento de um algoritmo professor de inglês utilizando técnicas de Processamento de Linguagem Natural (PNL) e Aprendizado de Máquina. O objetivo é criar um sistema capaz de entender consultas em inglês e fornecer respostas relevantes e informativas para auxiliar no aprendizado da língua inglesa.\n",
        "\n"
      ],
      "metadata": {
        "id": "adlTv8UAuaon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação do Ambiente:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "omBRc_yluwIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Instalando as dependencias >>"
      ],
      "metadata": {
        "id": "jvL8zyU-9E7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando as dependencias\n",
        "!pip install tensorflow transformers nltk pandas scikit-learn matplotlib seaborn wordcloud\n",
        "!pip install pyspellchecker\n",
        "!pip install textblob\n",
        "!pip install transformers pandas\n",
        "!pip install transformers torch\n",
        "!pip install transformers sklearn\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch] -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC1t4Y0Pu-x7",
        "outputId": "f7a1a674-3d93-4b36-8a2e-4ac453d38ee2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.29.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importando as bibliotecas >>"
      ],
      "metadata": {
        "id": "_lxEZbkA9Qmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bibliotecas para manipulação e análise de dados\n",
        "import pandas as pd  # Para manipulação de dados em formato de DataFrame\n",
        "import numpy as np   # Para operações numéricas eficientes em arrays\n",
        "\n",
        "# Bibliotecas para plotagem de gráficos\n",
        "import matplotlib.pyplot as plt  # Para plotagem de gráficos\n",
        "import seaborn as sns            # Para visualização estatística de dados\n",
        "\n",
        "# Bibliotecas para processamento de linguagem natural\n",
        "import nltk                      # Para processamento de linguagem natural\n",
        "from nltk.corpus import stopwords  # Lista de palavras comuns que serão ignoradas em análises de texto\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # Para tokenização de palavras e sentenças\n",
        "from nltk.stem import WordNetLemmatizer  # Para lematização de palavras\n",
        "\n",
        "# Ferramentas adicionais de NLP e tokenização\n",
        "from transformers import BertTokenizer, TFBertForQuestionAnswering  # Para uso de modelos BERT em NLP\n",
        "\n",
        "# Bibliotecas para aprendizado de máquina e processamento de modelos\n",
        "import tensorflow as tf             # Para construção de modelos de aprendizado profundo\n",
        "from tensorflow import keras       # Para construção de modelos de deep learning\n",
        "from sklearn.model_selection import train_test_split  # Para divisão de dados em treinamento e teste\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Para vetorização de texto em TF-IDF\n",
        "\n",
        "# Biblioteca para manipulação de texto e limpeza\n",
        "import re                         # Para manipulação de expressões regulares\n",
        "from collections import Counter  # Para contagem de elementos em uma lista\n",
        "from wordcloud import WordCloud  # Para criação de nuvens de palavras\n",
        "\n",
        "# Bibliotecas para comunicação com a web e parsing de HTML\n",
        "import requests                   # Para fazer requisições HTTP\n",
        "from bs4 import BeautifulSoup    # Para fazer parsing de HTML\n",
        "from urllib.parse import urlparse  # Para parsing de URLs\n",
        "\n",
        "# Biblioteca para interação com banco de dados\n",
        "import sqlite3                   # Para interação com bancos de dados SQLite\n",
        "\n",
        "# Biblioteca para interação com o Google Colab\n",
        "from google.colab import files, drive  # Para interação com o Google Colab\n",
        "\n",
        "# Biblioteca para correção ortográfica\n",
        "from spellchecker import SpellChecker  # Para correção ortográfica\n",
        "\n",
        "# Downloads e preparações necessárias para o NLTK\n",
        "nltk.download('punkt')      # Downloads adicionais para o NLTK\n",
        "nltk.download('stopwords')  # Downloads adicionais para o NLTK\n",
        "nltk.download('wordnet')    # Downloads adicionais para o NLTK\n",
        "\n",
        "# Bibliotecas para pré-processamento de texto para modelos de deep learning\n",
        "from keras.preprocessing.text import Tokenizer        # Para tokenização de texto\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "from textblob import TextBlob  # Para processamento de texto e análise de sentimentos\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Para vetorização de texto em contagens de palavras\n",
        "from sklearn.cluster import KMeans  # Para clustering de dados textuais\n",
        "from sklearn.metrics import silhouette_score  # Para avaliação de clusters\n",
        "from sklearn.cluster import DBSCAN, AgglomerativeClustering  # Para clustering hierárquico e DBSCAN\n",
        "from sklearn.decomposition import PCA           # Para redução de dimensionalidade com PCA\n",
        "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score  # Para avaliação de clusters\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import os\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "bsMyCcTtf77u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa23d248-c81c-4053-b65b-fc80ac304f9a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coleta e carregamento dos dados"
      ],
      "metadata": {
        "id": "cbCdAz7CoHnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  O conjunto de dados consiste em conteúdo textual de Wikibooks em inglês e português, organizado em capítulos e disponibilizado em formato de texto simples e HTML. Recriei o Dataset selecionando apenas estas duas linguagens a partir do conjunto original que continha o conteúdo em diversas outras linguagens.\n",
        "  \n",
        "Conjunto original em:\n",
        "https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset/data"
      ],
      "metadata": {
        "id": "3RDyw1XWtp5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho para o arquivo no Google Drive\n",
        "caminho_arquivo = '/content/drive/MyDrive/Data-Science_Analytics/Projetos/CB-AI English Teacher/wikibooks_pt_en_version.sqlite'\n",
        "\n",
        "# Conectar ao banco de dados\n",
        "conexao = sqlite3.connect(caminho_arquivo)\n",
        "\n",
        "# Cursor para executar comandos SQL\n",
        "cursor = conexao.cursor()\n",
        "\n",
        "# Consulta para listar todas as tabelas\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "tabelas = cursor.fetchall()\n",
        "\n",
        "# Exibir o nome de todas as tabelas encontradas\n",
        "print(\"Tabelas encontradas no banco de dados:\")\n",
        "for tabela in tabelas:\n",
        "    print(tabela[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftgtN53f6dx",
        "outputId": "b9739e9e-7046-4c83-e868-31a119914e47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Tabelas encontradas no banco de dados:\n",
            "pt\n",
            "en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise inicial dos dados"
      ],
      "metadata": {
        "id": "wFRwn0LsMque"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exibição de uma prévia do conteúdo dos Datasets >>"
      ],
      "metadata": {
        "id": "PPk1EpnXFhs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nossas tabelas relevantes\n",
        "tabela_1= 'pt'\n",
        "tabela_2= 'en'\n",
        "\n",
        "# Carregar os dados em Dataframe para manipulaçao\n",
        "query_1 = f\"SELECT * FROM {tabela_1};\"\n",
        "df1 = pd.read_sql_query(query_1, conexao)\n",
        "\n",
        "query_2 = f\"SELECT * FROM {tabela_2};\"\n",
        "df2 = pd.read_sql_query(query_2, conexao)\n",
        "\n",
        "# Fechar a conexão com o banco de dados original\n",
        "conexao.close()\n",
        "\n",
        "# Exibir as primeiras linhas para melhor compreesão do conteúdo\n",
        "print(\"\\nPrimeiras linhas do DataFrame 'pt':\")\n",
        "print(df1.head(20))\n",
        "print(\"\\nPrimeiras linhas do DataFrame 'en':\")\n",
        "print(df2.head(20))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DmqZSa4Kmvn",
        "outputId": "c54d4efe-c250-4458-bfc8-4b73fb5595b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7eadcc40b880>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Colunas do DataFrame 'pt' e 'en'\n",
        "\n",
        "title: Título do capítulo ou seção.\n",
        "url: URL do conteúdo.\n",
        "abstract: Resumo ou introdução do conteúdo.\n",
        "body_text: Texto principal do conteúdo.\n",
        "body_html: HTML do conteúdo principal.\n",
        "\n",
        "Análise:\n",
        "title: Útil para identificar o assunto de cada entrada, o que pode ser importante para classificar ou organizar os dados de treinamento por tema ou dificuldade.\n",
        "\n",
        "url: Embora possa ser útil para rastrear a origem do conteúdo ou verificar a integridade dos dados, provavelmente não será necessária para o treinamento do modelo de linguagem.\n",
        "\n",
        "abstract: Provavelmente útil, pois oferece uma breve descrição ou introdução ao tópico tratado, o que pode ser uma boa fonte de dados concisos para ensinar definições ou conceitos chave.\n",
        "\n",
        "body_text: Essencial, pois contém a maior parte do conteúdo educativo. Este texto é vital para o treinamento, pois fornece o contexto completo e o desenvolvimento dos temas abordados.\n",
        "\n",
        "body_html: Menos útil para o treinamento do modelo diretamente devido às tags HTML, mas pode ser importante para extrair texto formatado ou elementos específicos (como tabelas, listas ou seções destacadas), dependendo de como você deseja processar ou apresentar o material educativo."
      ],
      "metadata": {
        "id": "Bg28pU64scs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analisando a estrutura basica dos dados >>"
      ],
      "metadata": {
        "id": "N0DO-ZstF2MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para analisar a estrutura basica dos dados\n",
        "\n",
        "def analise_inicial_datasets(df_pt, df_en):\n",
        "    # Verificando linhas e colunas\n",
        "    print(\"Linhas e colunas da tabela 'pt':\", df_pt.shape)\n",
        "    print(\"Linhas e colunas da tabela 'en':\", df_en.shape)\n",
        "\n",
        "    # Verificando o comprimento médio dos textos\n",
        "    len_pt = df_pt['body_text'].apply(len).mean()\n",
        "    len_en = df_en['body_text'].apply(len).mean()\n",
        "    print(\"Comprimento médio dos textos em português:\", len_pt)\n",
        "    print(\"Comprimento médio dos textos em inglês:\", len_en)\n",
        "\n",
        "    # Analisando a diversidade de temas\n",
        "    un_topics_pt = df_pt['title'].nunique()\n",
        "    un_topics_en = df_en['title'].nunique()\n",
        "    print(\"Número total de tópicos únicos em português:\", un_topics_pt)\n",
        "    print(\"Número total de tópicos únicos em inglês:\", un_topics_en)\n",
        "\n",
        "    # Recursos educacionais em inglês\n",
        "    educational_resources_en = df_en[df_en['title'].str.contains('lesson', case=False)]\n",
        "    print(\"Número de recursos educacionais em inglês:\", len(educational_resources_en))\n",
        "\n",
        "    # Diversidade e origem dos dados\n",
        "    unique_urls_pt = df_pt['url'].nunique()\n",
        "    unique_urls_en = df_en['url'].nunique()\n",
        "    print(\"Número de fontes únicas em português:\", unique_urls_pt)\n",
        "    print(\"Número de fontes únicas em inglês:\", unique_urls_en)\n",
        "\n"
      ],
      "metadata": {
        "id": "ma7OZtNKu6Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando analise inicial nos Datasets\n",
        "\n",
        "analise_inicial_datasets(df1, df2)"
      ],
      "metadata": {
        "id": "XEBAUaYOvBFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processamento inicial básico"
      ],
      "metadata": {
        "id": "Vr9vqvY_wFcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deletar colunas irrelevantes\n",
        "columns_to_remove = ['body_html', 'url']\n",
        "\n",
        "df1.drop(columns=columns_to_remove, inplace=True)\n",
        "df2.drop(columns=columns_to_remove, inplace=True)\n",
        "\n",
        "# Salvar os dataframes\n",
        "df1.to_csv('cleaned_df1.csv', index=False)\n",
        "df2.to_csv('cleaned_df2.csv', index=False)\n",
        "print('Os arquivos.csv, foram salvos para os conjuntos de dados df1 e df2')\n",
        "\n"
      ],
      "metadata": {
        "id": "1G-B2gJ5te5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar duplicatas e remover se necessário\n",
        "def check_and_remove_duplicates(df, df_name):\n",
        "    initial_count = len(df)\n",
        "    df = df.drop_duplicates()\n",
        "    if len(df) < initial_count:\n",
        "        print(f\"Duplicatas removidas em {df_name}: {initial_count - len(df)} linhas.\")\n",
        "    else:\n",
        "        print(f\"Nenhuma duplicata encontrada em {df_name}.\")\n",
        "    return df\n",
        "\n",
        "df1 = check_and_remove_duplicates(df1, \"df1\")\n",
        "df2 = check_and_remove_duplicates(df2, \"df2\")\n",
        "\n",
        "# Função para limpar e formatar strings\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    cleaned_text = ' '.join(text.strip().split())\n",
        "    return cleaned_text\n",
        "\n",
        "# Aplicar limpeza em colunas de texto\n",
        "text_columns = ['body_text', 'abstract', 'title']\n",
        "for column in text_columns:\n",
        "    df1[column] = df1[column].apply(clean_text)\n",
        "    df2[column] = df2[column].apply(clean_text)\n",
        "    print(f\"Coluna '{column}' de df1 e df2 limpa e formatada.\")\n",
        "\n",
        "# Verificar e tratar valores nulos\n",
        "def check_and_handle_nulls(df, df_name):\n",
        "    for column in df.columns:\n",
        "        null_count = df[column].isnull().sum()\n",
        "        if null_count > 0:\n",
        "            print(f\"Valores nulos encontrados na coluna {column} do {df_name}: {null_count}\")\n",
        "            df[column].fillna('Valor Padrão', inplace=True)\n",
        "            print(f\"Valores nulos na coluna {column} do {df_name} tratados.\")\n",
        "        else:\n",
        "          print('Não foram encontrados valores nulos')\n",
        "\n",
        "check_and_handle_nulls(df1, \"df1\")\n",
        "check_and_handle_nulls(df2, \"df2\")\n",
        "\n",
        "# Salvar os DataFrames limpos\n",
        "df1.to_csv('cleaned_df1.csv', index=False)\n",
        "df2.to_csv('cleaned_df2.csv', index=False)\n",
        "print(\"DataFrames df1 e df2 foram limpos e salvos com sucesso.\")\n"
      ],
      "metadata": {
        "id": "tXBJct0e65mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar as primeiras linhas novamente para conferir o processamento inicial\n",
        "print(df1.head(10))\n",
        "print(df2.head(10))"
      ],
      "metadata": {
        "id": "h1s3liakzeUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando os dados:"
      ],
      "metadata": {
        "id": "D5I-2W_Ao7pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Criando graficos para visualizar a distribuição das principais categorias encontradas >>"
      ],
      "metadata": {
        "id": "FXINEWY-CRxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para extrair a categoria principal e subcategoria de cada título\n",
        "def extract_category(title):\n",
        "    parts = title.split(':')\n",
        "    if len(parts) > 1:\n",
        "        category_parts = parts[1].split('/')\n",
        "        main_category = category_parts[0].strip()\n",
        "        sub_category = category_parts[1].strip() if len(category_parts) > 1 else None\n",
        "        return main_category, sub_category\n",
        "    else:\n",
        "        return 'Outros', None\n",
        "\n",
        "# Aplica a função aos DataFrames para extrair as categorias principais e subcategorias\n",
        "df1[['main_category', 'sub_category']] = df1['title'].apply(lambda x: pd.Series(extract_category(x)))\n",
        "df2[['main_category', 'sub_category']] = df2['title'].apply(lambda x: pd.Series(extract_category(x)))\n",
        "\n",
        "# Cria gráficos de barras para as categorias principais e subcategorias em português e inglês\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='main_category', data=df1, order=df1['main_category'].value_counts().index[:30], palette='viridis')\n",
        "plt.title('Top 30 Categorias Principais em Português')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Categorias Principais')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='main_category', data=df2, order=df2['main_category'].value_counts().index[:30], palette='viridis')\n",
        "plt.title('Top 30 Categorias Principais em Inglês')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Categorias Principais')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='sub_category', data=df1.dropna(), order=df1['sub_category'].value_counts().index[:30], palette='viridis')\n",
        "plt.title('Top 30 Subcategorias em Português')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Subcategorias')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(y='sub_category', data=df2.dropna(), order=df2['sub_category'].value_counts().index[:30], palette='viridis')\n",
        "plt.title('Top 30 Subcategorias em Inglês')\n",
        "plt.xlabel('Frequência')\n",
        "plt.ylabel('Subcategorias')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I6ACvv-49LsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualizar comprimento dos textos >>\n",
        "\n",
        "Aplicação da transformação logarítmica nos comprimentos dos textos para lidar melhor com a dispersao dos dados,\n",
        "visto que os textos possuem uma grande diversidade de tamanhos."
      ],
      "metadata": {
        "id": "aGhZ8qBGAoV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_transform(x):\n",
        "    # Adiciona 1 antes de aplicar o log para evitar o logaritmo de zero.\n",
        "    return np.log(x + 1)\n",
        "\n",
        "# Aplica a transformação logarítmica nos comprimentos dos textos para os 2 Datasets\n",
        "log_lengths_pt = df1['body_text'].apply(len).apply(log_transform)\n",
        "log_lengths_en = df2['body_text'].apply(len).apply(log_transform)\n",
        "\n",
        "# Configura o estilo dos gráficos\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Histograma para a distribuição do comprimento dos textos em português2\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.histplot(log_lengths_pt, bins=30, color=\"blue\", kde=True)\n",
        "plt.title('Log Distribuição do Comprimento dos Textos em Português')\n",
        "plt.xlabel('Log Comprimento dos Textos')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "print()\n",
        "\n",
        "# Histograma para a distribuição do comprimento dos textos em inglês.\n",
        "plt.figure(figsize=(7, 3))\n",
        "sns.histplot(log_lengths_en, bins=30, color=\"green\", kde=True)\n",
        "plt.title('Log Distribuição do Comprimento dos Textos em Inglês')\n",
        "plt.xlabel('Log Comprimento dos Textos')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnbyF03kAj75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise dos histogramas de comprimento dos textos:\n",
        "\n",
        "Após a transformação logarítmica, os histogramas mostram que os dois conjuntos de dados seguem uma distribuição log-normal, mostrando que textos mais curtos são comuns, com uma cauda longa indicando textos significativamente mais longos. A presença destes textos longos pode indicar materiais mais detalhados ou avançados.\n",
        "\n",
        "Processamento e transformações:\n",
        "\n",
        "Segmentar textos mais longos em unidades menores mais gerenciáveis. Utilizar técnicas como truncamento, padding ou divisão de textos longos em várias partes para que o comprimento dos textos de entrada seja consistente.\n"
      ],
      "metadata": {
        "id": "8lKQSmwR0Erf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processamento dos Dados:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sIYHgV0Lu_yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pré processamento avançado de texto >>"
      ],
      "metadata": {
        "id": "n5nxVCmp-uQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processa o texto para tokenização e limpeza básica.\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Separa pontuações dos tokens sem remover, pois são importantes gramaticalmente.\n",
        "    text = re.sub(r'([,.!?])', r' \\1 ', text)\n",
        "    # Substitui múltiplos espaços por um.\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Segmenta o texto em sentenças.\n",
        "def segment_text(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "# Codifica textos em sequências numéricas e aplica padding.\n",
        "def encode_and_pad(texts, max_length=100, vocab_size=30000, oov_token=\"<OOV>\"):\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Aplica pré-processamento nas colunas especificadas.\n",
        "def apply_preprocessing(df, text_columns):\n",
        "    for column in text_columns:\n",
        "        df[f'cleaned_{column}'] = df[column].apply(preprocess_text)\n",
        "        df[f'segmented_{column}'] = df[f'cleaned_{column}'].apply(segment_text)\n",
        "    return df\n",
        "\n",
        "# Processa dois DataFrames para aplicar pré-processamento de texto.\n",
        "def process_dataframes(df1, df2, text_column):\n",
        "    df1 = apply_preprocessing(df1, [text_column])\n",
        "    df2 = apply_preprocessing(df2, [text_column])\n",
        "    return df1, df2\n",
        "\n",
        "\n",
        "text_column = 'body_text'\n",
        "df1, df2 = process_dataframes(df1, df2, text_column)\n",
        "\n",
        "print(\"Processed DataFrame 1:\", df1)\n",
        "print(\"Processed DataFrame 2:\", df2)\n",
        "\n"
      ],
      "metadata": {
        "id": "jp-tz0bRQZFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar uma coluna 'language' para cada DataFrame antes da concatenação\n",
        "df1['language'] = 'Portuguese'\n",
        "df2['language'] = 'English'\n",
        "\n",
        "# Concatenando os dados\n",
        "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# Verificar as primeiras linhas para confirmar a concatenação e a coluna de idioma\n",
        "print(df_combined.head())\n"
      ],
      "metadata": {
        "id": "8fYS4u2nvPXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Salvando o conjunto de dados totalmente processado >>"
      ],
      "metadata": {
        "id": "SvrhfIDFxQlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar df1 e df2 em arquivos CSV\n",
        "df_combined.to_csv('df_processado_combinado.csv', index=False)\n",
        "print(' O conjunto de dados combinado, foi salvo no diretório atual')\n",
        "\n",
        "# Caminho para salvar os arquivos CSV no Google Drive\n",
        "path_df_combined = '/content/drive/MyDrive/Data-Science_Analytics/Projetos/CB-AI English Teacher/df1_processado_combinado.csv'\n",
        "\n",
        "df_combined.to_csv(path_df_combined, index=False)\n",
        "print(f'O conjunto de dados combinado foi salvo em: {path_df_combined}')\n"
      ],
      "metadata": {
        "id": "ddMdKD5Gr-H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Definição e Treinamento do Modelo:\n",
        "\n"
      ],
      "metadata": {
        "id": "jQc5XuyWvI6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A escolha GPT e os modelos Transformers, foram devido a questão de desempenho superior em processamento de linguagem natural, capacidade de aprendizado contextual, transferência de aprendizado, e disposição de modelos pré treinados.\n",
        "\n"
      ],
      "metadata": {
        "id": "o0VZuTwrnftg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Divisão do conjunto de dados entre treino e teste >>\n",
        "\n"
      ],
      "metadata": {
        "id": "alNkEsQkw2gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Carregar os dados processados anteriormente\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data-Science_Analytics/Projetos/CB-AI English Teacher/df1_processado_combinado.csv')\n",
        "\n",
        "# Verificar conteúdo\n",
        "print(df.head(10))\n",
        "\n",
        "# Dividir os dados em conjuntos de treino e teste, com 20% dos dados para teste e semente de reprodutibilidade\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED4jNd-BufQ4",
        "outputId": "8ffc3d7f-d72c-444f-9789-dd4f8903c5e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               title  \\\n",
            "0   Wikilivros: Livro de receitas/Massa para empadas   \n",
            "1           Wikilivros: Escotismo no Brasil/O que é?   \n",
            "2              Wikilivros: Guia do Rugby/Leis/O Maul   \n",
            "3  Wikilivros: Matemática elementar/Geometria pla...   \n",
            "4            Wikilivros: Português para estrangeiros   \n",
            "5              Wikilivros: Estruturas metálicas/Capa   \n",
            "6  Wikilivros: Mecânica dos fluidos/Exercícios re...   \n",
            "7  Wikilivros: WikiRPG/Lista de fobias e seu uso ...   \n",
            "8  Wikilivros: ITIL V3/Estratégia de serviço/Conc...   \n",
            "9                        Wikilivros: Pintura/Cultura   \n",
            "\n",
            "                                            abstract  \\\n",
            "0               Existem várias maneiras de preparar:   \n",
            "1                             * O que é? Imagem:25%.   \n",
            "2  O Maul acontece quando três jogadores, sendo u...   \n",
            "3                                    == Paralelas ==   \n",
            "4                        Português para estrangeiros   \n",
            "5                               Estruturas Metálicas   \n",
            "6                                           * Teoria   \n",
            "7  Fobia em WikiRPG é o temor ou aversão exagerad...   \n",
            "8  Definido em termos de resultado de negócios pe...   \n",
            "9  Uma parte da história da pintura na arte orien...   \n",
            "\n",
            "                                           body_text  \\\n",
            "0  acima: <<< Índiceanterior: <<< Salgados, Lanch...   \n",
            "1  O que é? O que é escotismo? O escoteiro Propós...   \n",
            "2  O Maul acontece quando três jogadores, sendo u...   \n",
            "3  Índice 1 Paralelas 2 Perpendiculares 3 Feixe d...   \n",
            "4  Etapas de desenvolvimento - 9 fases Início: Bá...   \n",
            "5                               Estruturas Metálicas   \n",
            "6  Teoria Voltar para a lista de exercícios resol...   \n",
            "7  Fobia em WikiRPG é o temor ou aversão exagerad...   \n",
            "8  Definido em termos de resultado de negócios pe...   \n",
            "9  Uma parte da história da pintura na arte orien...   \n",
            "\n",
            "                 main_category                          sub_category  \\\n",
            "0            Livro de receitas                    Massa para empadas   \n",
            "1          Escotismo no Brasil                              O que é?   \n",
            "2                Guia do Rugby                                  Leis   \n",
            "3         Matemática elementar                       Geometria plana   \n",
            "4  Português para estrangeiros                                   NaN   \n",
            "5         Estruturas metálicas                                  Capa   \n",
            "6         Mecânica dos fluidos                 Exercícios resolvidos   \n",
            "7                      WikiRPG  Lista de fobias e seu uso em WikiRPG   \n",
            "8                      ITIL V3                 Estratégia de serviço   \n",
            "9                      Pintura                               Cultura   \n",
            "\n",
            "                                   cleaned_body_text  \\\n",
            "0  acima : < < < Índiceanterior : < < < Salgados ...   \n",
            "1  O que é ? O que é escotismo ? O escoteiro Prop...   \n",
            "2  O Maul acontece quando três jogadores , sendo ...   \n",
            "3  Índice 1 Paralelas 2 Perpendiculares 3 Feixe d...   \n",
            "4  Etapas de desenvolvimento - 9 fases Início : B...   \n",
            "5                               Estruturas Metálicas   \n",
            "6  Teoria Voltar para a lista de exercícios resol...   \n",
            "7  Fobia em WikiRPG é o temor ou aversão exagerad...   \n",
            "8  Definido em termos de resultado de negócios pe...   \n",
            "9  Uma parte da história da pintura na arte orien...   \n",
            "\n",
            "                                 segmented_body_text    language  \n",
            "0  ['acima : < < < Índiceanterior : < < < Salgado...  Portuguese  \n",
            "1  ['O que é ?', 'O que é escotismo ?', 'O escote...  Portuguese  \n",
            "2  ['O Maul acontece quando três jogadores , send...  Portuguese  \n",
            "3  ['Índice 1 Paralelas 2 Perpendiculares 3 Feixe...  Portuguese  \n",
            "4  ['Etapas de desenvolvimento - 9 fases Início :...  Portuguese  \n",
            "5                           ['Estruturas Metálicas']  Portuguese  \n",
            "6  ['Teoria Voltar para a lista de exercícios res...  Portuguese  \n",
            "7  ['Fobia em WikiRPG é o temor ou aversão exager...  Portuguese  \n",
            "8  ['Definido em termos de resultado de negócios ...  Portuguese  \n",
            "9  ['Uma parte da história da pintura na arte ori...  Portuguese  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definição do modelo e ajuste dos dados >>"
      ],
      "metadata": {
        "id": "5-F-syl1wNz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bibliotecas utilizadas:\n",
        "\n",
        "* transformers: Biblioteca de modelos de deep learning baseados em arquiteturas de Transformer. Utilizada para processamento de linguagem natural, como tradução automática, geração de texto e análise de sentimentos. Transformers suporta TensorFlow e PyTorch.\n",
        "* torch: É a biblioteca principal do PyTorch, usada para todas as operações com tensores e deep learning.\n",
        "* DataLoader, TensorDataset: Ferramentas do PyTorch para manipulação de dados em lotes durante o treinamento.\n",
        "* pad_sequence: Função do PyTorch para padronizar o comprimento dos tensores, essencial para o processamento em lotes.\n",
        "* GPT2Tokenizer, GPT2Model: Componentes da biblioteca transformers que fornecem o modelo GPT-2 pré-treinado e seu tokenizador.\n",
        "* pandas: Biblioteca para manipulação de dados, utilizada para operar com DataFrames.\n",
        "\n"
      ],
      "metadata": {
        "id": "5Zg4cem_DF0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Carregar o tokenizador e o modelo GPT-2 com uma 'language modeling head'\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Verificar e definir um token de padding se não estiver presente\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))  # Ajustar o modelo para o novo tokenizer\n",
        "\n",
        "# Classe personalizada para o Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        if not isinstance(text, str):\n",
        "            text = \"\"  # Assegurar que o texto é uma string\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        labels = encoding['input_ids'].roll(-1, dims=1)\n",
        "        labels[:, -1] = -100  # Usar um índice de ignorar na última posição\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': labels.squeeze(0)\n",
        "        }\n",
        "\n",
        "# Definindo a coluna de texto dos Dataframes\n",
        "train_texts = train_df['cleaned_body_text'].tolist()\n",
        "test_texts = test_df['cleaned_body_text'].tolist()\n",
        "\n",
        "# Criar instâncias do Dataset personalizado\n",
        "train_dataset = TextDataset(train_texts, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, tokenizer)\n",
        "\n",
        "# Criar DataLoader para gerenciar os lotes\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eUxIA01-Tck",
        "outputId": "7287e966-b555-4a0c-df3a-a73782bf2b92"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinamento do modelo utilizando os dados preparados >>"
      ],
      "metadata": {
        "id": "HEib3JM9w80j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Definição dos argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Configuração do Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Treinar o modelo\n",
        "trainer.train()\n",
        "\n",
        "# Avaliar o modelo\n",
        "results = trainer.evaluate()\n",
        "print(\"Avaliação do modelo:\", results)\n"
      ],
      "metadata": {
        "id": "nDZc4n2i-Z-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Avaliação e ajuste do Modelo:\n",
        "\n"
      ],
      "metadata": {
        "id": "r4JS3UqevTlu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcqYexCZo8Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geração de respostas em linguagem natural"
      ],
      "metadata": {
        "id": "1BvkypdTSOSO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OneQvhQGSKOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Interação com o Usuário:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mwMdT5g9vZB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementação de uma interface de usuário para interação com o algoritmo professor de inglês.\n"
      ],
      "metadata": {
        "id": "y-iiAl5NxP8s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uLkq-7Bvvcie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Processamento de consultas do usuário e geração de respostas."
      ],
      "metadata": {
        "id": "LPOKdQMTxVhj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrw1RCsSxWCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classificar niveis de dificuldade >>"
      ],
      "metadata": {
        "id": "rtSKf8ATRUL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classificar_dificuldade(df, coluna_texto='body_text', n_clusters=3):\n",
        "    \"\"\"\n",
        "    Classifica os textos do DataFrame em níveis de dificuldade usando o algoritmo KMeans.\n",
        "\n",
        "    Parâmetros:\n",
        "        df (pandas.DataFrame): DataFrame contendo os dados.\n",
        "        coluna_texto (str): Nome da coluna no DataFrame que contém os textos a serem classificados.\n",
        "        n_clusters (int): Número de níveis de dificuldade (clusters) desejados.\n",
        "\n",
        "    Retorna:\n",
        "        pandas.DataFrame: DataFrame com uma nova coluna 'nivel_dificuldade', indicando o\n",
        "                          nível de dificuldade de cada texto, com base na clusterização.\n",
        "    \"\"\"\n",
        "    # Configuração do TF-IDF Vectorizer\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "    # Transformação dos textos em uma matriz TF-IDF\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(df[coluna_texto])\n",
        "\n",
        "    # Configuração e aplicação do KMeans para classificar os textos em níveis de dificuldade\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=1)\n",
        "    df['nivel_dificuldade'] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    # Impressão da distribuição dos níveis de dificuldade\n",
        "    print(f\"Distribuição dos níveis de dificuldade dos textos:\\n{df['nivel_dificuldade'].value_counts()}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "cBYvAKmBfD-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Usando métricas para avaliação do desempenho do modelo\n"
      ],
      "metadata": {
        "id": "XV0dtqFjxHZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Carregar o modelo e o tokenizador do GPT-2\n",
        "model_name = 'gpt2'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2Model.from_pretrained(model_name)\n",
        "\n",
        "# Função para codificar os textos em formato compatível com o modelo\n",
        "def encode_texts(tokenizer, texts):\n",
        "    return [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=512) for text in texts]\n",
        "\n",
        "# Carregar dados de um arquivo CSV\n",
        "df = pd.read_csv('path_to_your_data.csv')\n",
        "\n",
        "# Dividir os dados em conjuntos de treino e teste\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preparar os dados de texto e seus respectivos rótulos\n",
        "train_texts = train_df['text_column'].tolist()  # Substitua 'text_column' pelo nome real da coluna de texto\n",
        "train_labels = train_df['label_column'].tolist()  # Substitua 'label_column' pelo nome real da coluna de rótulo\n",
        "test_texts = test_df['text_column'].tolist()\n",
        "test_labels = test_df['label_column'].tolist()\n",
        "\n",
        "# Codificar os textos\n",
        "train_encoded = encode_texts(tokenizer, train_texts)\n",
        "test_encoded = encode_texts(tokenizer, test_texts)\n",
        "\n",
        "# Configuração da validação cruzada para avaliação robusta\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, train_encoded, train_labels, cv=kf, scoring='accuracy')\n",
        "\n",
        "# Imprimir a acurácia média obtida pela validação cruzada\n",
        "print(\"Acurácia média com validação cruzada:\", scores.mean())\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "# Assumindo que você tem uma função `predict` para fazer previsões com seu modelo\n",
        "test_predictions = model.predict(test_encoded)  # Ajuste essa linha conforme necessário\n",
        "\n",
        "# Calcular e imprimir métricas de desempenho\n",
        "accuracy = accuracy_score(test_labels, test_predictions)\n",
        "report = classification_report(test_labels, test_predictions)\n",
        "\n",
        "print(\"Acurácia no conjunto de teste:\", accuracy)\n",
        "print(\"Relatório de classificação no conjunto de teste:\\n\", report)\n",
        "\n",
        "# Análise de erros (opcional)\n",
        "errors = test_labels != test_predictions\n",
        "error_df = test_df[errors]\n",
        "print(\"Exemplos de erros de classificação:\", error_df.head())\n"
      ],
      "metadata": {
        "id": "fYLyUPByvYsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Análise dos resultados"
      ],
      "metadata": {
        "id": "wNCuSh7GxKlY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mjyPnCIjxLWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes e Validação:\n",
        "\n"
      ],
      "metadata": {
        "id": "l8mzJRiovdA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testes do sistema em diferentes cenários e condições.\n",
        "\n"
      ],
      "metadata": {
        "id": "9p_nDUDGxe5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w8KswYKivf7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validação da eficácia e precisão das respostas geradas."
      ],
      "metadata": {
        "id": "SR8CIgkFxgic"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_wzF87kxhz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integração e Implantação:\n",
        "\n"
      ],
      "metadata": {
        "id": "ByZbIa9evgv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integração do algoritmo professor de inglês em uma aplicação ou plataforma de ensino de inglês.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNoYMjjNxlXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implantação do sistema para uso em produção."
      ],
      "metadata": {
        "id": "l39x0CZExnTp"
      }
    }
  ]
}